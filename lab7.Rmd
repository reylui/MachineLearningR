---
title: "lab7"
author: "Luis Jose Ramirez"
date: "March 7, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Lab 7 K-means with SNS data

The data we are using are comes from a random sample of 30000 high school students in the US from a well known SNS site. Given that this data set is relatively large we have a large demographic and may be able to gain insights to better improve our understanding of this group of people. This could potentially help marketing and advertising towards this demographic. 

The method we are using to analyze such patterns is called **clustering** in machine learning and we will start with the method known as **k-means**. We will go through this process from start to finish. 

### Collecting the data

The text message data has already been text mined and somewhat cleaned up.

### Exploring the data

1. Loading in the data, **stringAsFactors = TRUE** is not necessary since we have already measured the counts of each relevant word in each post. 

```{r, echo = TRUE}
snsdata <- read.csv("C:/Users/LuisJ/OneDrive/Desktop/RCode/MachineLearningR/data sets/snsdata.csv")
```

2. We observe the structure of the data

```{r, echo = TRUE}
str(snsdata)

```
3. We notice that there are more females than males, additionally there are missing values for age and gender. 

4. The problem with age is that we receive a range of ages from 3 to 100, when we are really looking at the high school demographic only. Since we are concerned with a particular demographic, specifically high school students to early adult, we want to consider only that data. We observe the following. Also we notice that there are many missing genders that account for about 17 percent.

```{r, echo = TRUE}
summary(snsdata$age)
table(snsdata$gender, useNA = "ifany")

```
5. Since we don't want all ages between 3 and 106, we can use an if else() statement to keep only the signify data.
```{r, echo = TRUE}
snsdata$age <- ifelse(snsdata$age >= 13 & snsdata$age < 20, snsdata$age, NA)
summary(snsdata$age)
```
6. We make up for missing genders by creating a dummy variable. The code we write does the following: assign 1 if the gender is female and not equal to NA, and 0 otherwise. The second one assigns value of 1 if the gender is missing a value and 0 otherwise. This way, two binary categorical feature variables are created to represent identity of each female or no gender, if both of these dummies are 0, then we have a male gender. 

```{r, echo = TRUE}
snsdata$female <- ifelse(snsdata$gender == "F" & !is.na(snsdata$gender), 1, 0)
snsdata$no_gender <- ifelse(is.na(snsdata$gender), 1, 0)

table(snsdata$gener, useNA = "ifany")
table(snsdata$female, useNA = "ifany")
table(snsdata$no_gender, useNA = "ifany")

```
### Data preperation - inputing the missing values

```{r, echo = TRUE}
mean(snsdata$age, na.rm = TRUE)
```
7. We note that this returns the mean of all the students, we are interested in using there graduation year to estimate their age.

8. So, by using the aggregate function we can do exactly this.
```{r, echo = FALSE}
aggregate(data = snsdata, age~gradyear, mean, na.rm = TRUE)
```

If there are missing values in the specified element, we instead add the average age for students in that particular graduation year, this can be done using the following code.
```{r, echo = TRUE}
ave_age <- ave(snsdata$age, snsdata$gradyear, FUN = function(x) mean(x, na.rm = TRUE))

snsdata$age <- ifelse(is.na(snsdata$age), ave_age, snsdata$age)

#checking the changes
summary(snsdata$age)
```

### Training a model on the data

10. We  will build our cluster by analyzing on 36 features that represent the desired number of various interest appeared on the tins profiles. 

11. Applying a z-score normalization to eliminate bias that is generated by having huger weight calculations for features with a larger range.


12. The changes are done using the following lines of code.
```{r, echo = TRUE}
interest <- snsdata[5:40]
interest_z <- as.data.frame(lapply(interest, scale))
summary(interest_z[5:10])
```

13. Training the model

```{r, echo = TRUE}
set.seed(12345) 

teen_clusters <- kmeans(interest_z, 5)
```

### Evaluating model performance

14. looking at the clusters 

```{r, echo = TRUE}
teen_clusters$size
teen_clusters$centers

```
16. We observe the size of each cluster along with the centers, the cluster center values are the cluster centro id coordinates. For example, if we look at the values of the first cluster, we see that the values are always above average for all the sports, suggesting we found a group of sports minded people, they also like shopping, abercrombie and religion. Mainly suggesting a middle to upper class christian families in the U.S. This can help marketers identify the main interest of this particular demographic for selling and advertising products.

### Model performance improvement

17. We can append the data cluster to the rest of the data set as follows.

```{r, echo = TRUE}
snsdata$cluster <- teen_clusters$cluster

snsdata[1:5, c("cluster", "gender", "age", "friends" )]
```
We can look at the various feature by clusters, so let see if we can make any interesting observations

```{r, echo = TRUE}
aggregate(data = snsdata, age~cluster, mean)
```
when we look at the age, we font notice a pattern that suggest a specific demographic yet, lets observe gender and friends next.

```{r, echo = TRUE}
aggregate(data = snsdata, female~cluster, mean)
aggregate(data = snsdata, friends~cluster, mean)
```

We notice from the previous cluster data in relation to the friends and gender that their cluster 1 is mostly female and have the biggest friend groups, along with an increase likeliness of being middle to upper class and religious. This algorithm is effective in identifying clusters.



















